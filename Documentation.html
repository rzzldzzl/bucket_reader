<html><body>
            <h1>Contents</h1>
            <ol>
                <li><a href="#requirements">Requirements</a></li>
                <li><a href="#getting_started">Getting Started</a></li>
                <li><a href="#manual">Bucket Reader User Manual</a>
                    <ol type="A">
                        <li><a href="#overview">Overview</a></li>
                        <li><a href="#configuration">Configuration</a></li>
                    </ol>
                </li>
                <li><a href="#examples">Examples</a>
                    <ol type="A">
                        <li><a href="#hadoop_app">A simple Hadoop application</a></li>
                        <li><a href="#hive">Usage with Hive</a></li>
                        <li><a href="#pig">Usage with Pig</a></li>
                    </ol>
                </li>
            </ol>

            <h1 id="requirements">Requirements</h1>

            <p>Bucket Reader requires Java Virtual Machine version 1.6 or later.</p>

            <p>Bucket Reader is meant to be used with Hadoop. It is compatible with both Hadoop 1.0 and Hadoop 2.0/Yarn.</p>

            <h1 id="getting_started">Getting Started</h1>

            <p>Visit the <a href="?https://github.com/rzzldzzl/bucket_reader/blob/master/Downloads.html">Downloads tab</a> to:</p>
            <ul>
                <li>Get the Bucket Reader jar file.</li>
                <li>View the Javadoc documentation for the Bucket Reader classes.</li>
                <li>Get the example application described below.</li>
            </ul>

            <p>To compile a Bucket Reader application (either the example application provided here, or your own) you
                must add the following to your classpath:
                <ul>
                    <li>The Bucket Reader jar.</li>
                    <li>The Hadoop jars. See the Hadoop
                        <a href="http://hadoop.apache.org/" target="_blank">project page</a> for details.</li>
                </ul>
            </p>

            <p>To compile the example application, you must add the open-source library OpenCSV to your classpath as
                well. See the Downloads tab for a link to the project page.</p>

            <h1 id="manual">Bucket Reader User Manual</h1>

            <h2 id="overview">Overview</h2>

            <p>This app allows you to read data in Splunk buckets using 3rd-party Hadoop-based applications. When you
                use the Hunk archiving functionality to export your raw-data journal files to HDFS, you can immediately
                query and analyze that data via Hunk. </p>

            <p>But what if you want to also view that data via applications like Hive or Pig? This app provides Hadoop
                tool classes that allow you to do just that:</p>

            <ul>
                <li>Provides classes that implement both the "old-style" and new "new-style" Hadoop interfaces, based on
                    the org.apache.hadoop.mapred and org.apache.hadoop.mapreduce packages.</li>
                <li>Provides access to index-time fields only - search-time fields are not available.</li>
                <li>Lets you configure which fields are retrieved as keys and as values.</li>
                <li>Formats returned data as TSV.</li>
            </ul>

            <p>This package provides both InputFormat and RecordReader classes that allow users to access
                Splunk-formatted data. Because some 3rd-party apps require "old-style" versions of these classes, and
                some require "new-style" versions, both are provided here:</p>

            <p>"Old-style":<br />
            <b><kbd>com.splunk.journal.hadoop.mapred.JournalInputFormat<br />
            com.splunk.journal.hadoop.mapred.JournalRecordReader</kbd></b>
            </p>

            <p>"New-style":<br />
            <b><kbd>com.splunk.journal.hadoop.mapreduce.JournalInputFormat<br />
            com.splunk.journal.hadoop.mapreduce.JournalRecordReader</kbd></b>
            </p>

            <h2 id="configuration">Configuration</h2>
            <p>Both versions are configured in the same way, and produce the same output. Configuration changes are
                handled via properties in the Configuration object for the Hadoop job.</p>

            <ul>
                <li>One key-value pair is read for each Splunk event.</li>
                <li>Both the key and the value are TSV-formatted by default. That is, fields will be surrounded by
                    double quotes ('"'), and delimited by tab characters.</li>
                <li>Any characters within a field that need to be escaped are proceeded by a back-slash ('\').</li>
                <li>All keys and values are UTF-8 encoded text.</li>
            </ul>

            <p>The fields which may be read include most index-time fields available via Splunk SPL (ie. search
                processing language). In addition, the field eventOffsetInFile is available, which indicates the
                byte-offset in the uncompressed raw-data file at which the event was read.</p>

            <p>Note that search-time fields are not available. See the configuration properties below for
                information on selecting fields.</p>

            <p>Available configurations properties:</p>

            <ul>
                <li><b><kbd>com.splunk.journal.hadoop.separator_char:</kbd></b><br />
                The character used to delimit fields in both keys and values. Must be a single character. Default is a
                    tab character.</li>

                <li><b><kbd>com.splunk.journal.hadoop.escape_char:</kbd></b><br />
                The character used to escape characters within fields. Delimiter characters and double quote characters
                    are escaped. Default is '\'.</li>

                <li><b><kbd>com.splunk.journal.hadoop.key_format:</kbd></b><br />
                The list of fields included in the key for each event read. Must be a comma-separated list of field
                    names. If a field is not available for a given event, that cell in the TSV text is represented by
                    the empty string. Any indexed field may be used, but fields that would be extracted by Splunk at
                    search-time are not available. The default value is "eventOffsetInFile".</li>

                <li><b><kbd>com.splunk.journal.hadoop.value_format:</kbd></b><br />
                The list of fields included in the value for each event read. Acceptable values are the same as for
                    key_format. The default value is "_time,_indextime,sourcetype,host,source,_raw".</li>

                <li><b><kbd>com.splunk.journal.hadoop.suppress.exceptions:</kbd></b><br />
                If false, an exception will be thrown whenever the data does not conform to the Splunk rawdata journal
                    file format. If true, the RecordReader will log the issue and treat the stream as if it has
                    ended. This can be useful if the application needs to scan the entire directory structure of an
                    archive index, as not all files will be journal files. Note that it is more efficient to instead
                    filter for files named "journal.gz". Any reasonable boolean value will be accepted
                    (e.g. true, True, t, y, 1,...). The default value is false.</li>
            </ul>

            <h1 id="examples">Examples</h1>

            <h2 id="hadoop_app">A simple Hadoop application</h2>

            <p>Let's look at some code for a Hadoop application that uses these classes. You may wish to download either
                the <a href="https://raw.githubusercontent.com/rzzldzzl/bucket_reader/master/examples/bucket-reader-examples-source.tar.gz">source-code</a> or
                the <a href="https://raw.githubusercontent.com/rzzldzzl/bucket_reader/master/examples/examples/bucket-reader-examples.jar">compiled classes</a> for
                this example first. Like the classic word-count example, this application counts the number of times
                each distinct value occurs for each field host, source, and sourcetype. To parse the values, we use the
                open-source library OpenCSV (see the project page at
                <a href="http://opencsv.sourceforge.net/" target="_blank">http://opencsv.sourceforge.net/</a>).</p>

            <p>First, we need a mapper class. The mapper receives fields read by the RecordReader. For each event, it
                emits three pairs, one for each of the fields: host, source, and sourcetype. The key for each pair is a
                string indicating the field and its value. The value of the pair will be the number "1".</p>

<pre><code>
package com.splunk.journal.hadoop;

import com.opencsv.CSVParser;
//import au.com.bytecode.opencsv.CSVParser;
import com.splunk.journal.RawdataJournalReader;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;

import java.io.IOException;
import java.util.Arrays;

/**
 * Reads in a raw-data journal file. For each event, it emits each of the following pairs:
 * "host" -&gt; host value from the event
 * "source" -&gt; source value from the event
 * "sourcetype" -&gt; sourcetype value from the event
 */
public class JournalMapper extends Mapper&lt;Text, Text, Text, LongWritable&gt; {
    Logger logger = Logger.getLogger(JournalMapper.class);

    CSVParser csvParser;
    String[] fieldNames;
    int hostIdx = -1;
    int sourceIdx = -1;
    int sourcetypeIdx = -1;

    Text fieldPair = new Text();
    final static LongWritable ONE = new LongWritable(1);

    public JournalMapper() {
    }

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        super.setup(context);
        Configuration conf = context.getConfiguration();
        csvParser = new CSVParser(RecordReaderHelper.getSeparatorChar(conf), '"',
                RecordReaderHelper.getEscapeChar(conf));

        fieldNames = RecordReaderHelper.getValueFieldNames(conf);
        for (int i = 0; i &lt; fieldNames.length; i++) {
            if (fieldNames[i].equals("host"))
                hostIdx = i;

            else if (fieldNames[i].equals("source"))
                sourceIdx = i;

            else if (fieldNames[i].equals("sourcetype"))
                sourcetypeIdx = i;
        }
        logger.info("Looking for host in column " + hostIdx);
        logger.info("Looking for source in column " + sourceIdx);
        logger.info("Looking for sourcetype in column " + sourcetypeIdx);
    }

    @Override
    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
        byte[] valBytes = value.getBytes();
        String line = new String(valBytes, 0, value.getLength(), RawdataJournalReader.UTF_8);
        String[] fields = null;
        try {
            fields = csvParser.parseLine(line);
        }
        catch (IOException ex) {
            logger.error("Found unparsable line '" + line + "'", ex);
        }
        if (fields.length != fieldNames.length)
            logger.warn("Wrong number of fields received! Expected fields = " + Arrays.toString(fieldNames)
                    + ". Found values = " + Arrays.toString(fields));
        else {
            if (hostIdx &gt; -1) {
                fieldPair.set("host = " + fields[hostIdx]);
                context.write(fieldPair, ONE);
            }
            if (sourceIdx &gt; -1) {
                fieldPair.set("source = " + fields[sourceIdx]);
                context.write(fieldPair, ONE);
            }
            if (sourcetypeIdx &gt; -1) {
                fieldPair.set("sourcetype = " + fields[sourcetypeIdx]);
                context.write(fieldPair, ONE);
            }
        }
    }
}
</code></pre>

            <p>Note that in the <b><kbd>setup</kbd></b> method, the mapper uses the method
                <b><kbd>RecordReaderHelper.getValueFieldNames</kbd></b> to find what positions our three fields of
                interest are in. This allows us to re-use the app with different field configurations.</p>

            <p>Next, we need a reducer class. The reducer creates a map whose keys are the keys emitted from the mapper,
                and whose values are the sums of the values associated with each key. </p>

<pre><code>
package com.splunk.journal.hadoop;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 * Sums the values associated with each key.
 */
class JournalReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; {
    LongWritable count = new LongWritable();

    @Override
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException {
        Map&lt;String, Long&gt; valueToCount = new HashMap&lt;String, Long&gt;();
        long sum = 0;
        for (LongWritable number : values) {
            sum += number.get();
        }
        count.set(sum);
        context.write(key, count);
    }

}
</code></pre>

<p>Now we need a class that wires together our mapper and reducer classes, along with the appropriate inputs and outputs.</p>

<pre><code>
package com.splunk.journal.hadoop;

import com.splunk.journal.hadoop.mapreduce.JournalInputFormat;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;
import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.log4j.Logger;


import java.io.File;
import java.io.IOException;
import java.util.Collections;

/**
 * A simple example to demonstrate use of &lt;code&gt;JournalInputFormat&lt;/code&gt; and (implicitly)
 * &lt;code&gt;RawdataJournalReader&lt;/code&gt;. To run, make sure that the Bucket Reader jar is in your classpath, in addition to
 * appropriate 3rd party jars.
 */
public class SplunkRecordReaderExample {
    static Logger logger = Logger.getLogger(SplunkRecordReaderExample.class);

    public Job createJob(String inputPath, String outputPath) throws IOException {
        Configuration configuration = new Configuration();

        // This demonstrates changing the value-format of the reader. Any format that includes these three fields would
        // work.
        configuration.set("com.splunk.journal.hadoop.value_format", "host,source,sourcetype");

        Job job = new Job(configuration, "Counting host, source, sourcetype");
        job.setJarByClass(SplunkRecordReaderExample.class);
        job.setInputFormatClass(JournalInputFormat.class);

        job.setMapperClass(JournalMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);

        job.setReducerClass(JournalReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.addInputPath(job, new Path(inputPath));
        FileOutputFormat.setOutputPath(job, new Path(outputPath));

        return job;
    }

    public static void main(String args[]) throws IOException, InterruptedException {
        String inputPath = args[0];
        String outputPath = args[1];

        FileUtil.fullyDelete(new File(outputPath));

        SplunkRecordReaderExample exampleApp = new SplunkRecordReaderExample();
        Job job = exampleApp.createJob(inputPath, outputPath);
        JobControl controller = new JobControl("Field counting");
        controller.addJob(new ControlledJob(job, Collections.EMPTY_LIST));
        controller.run();

        try {
            while (!controller.allFinished()) {
                logger.info("Waiting on search...");
                Thread.sleep(5000);
            }
        }
        finally {
            System.exit(0);
        }
    }
}
</code></pre>

<p>This code indicates to Hadoop that we want to use the Bucket Reader classes with this line:</p>
<pre><code>job.setInputFormatClass(JournalInputFormat.class);</code></pre>

<p>The app is invoked with two arguments. The first is a path to a Splunk raw-data journal file (which will likely be named "journal.gz"). The second is a path to where the output should go. Running this app on a journal file that contained data from the Splunk tutorial produced these results:</p>

<samp><pre>
host = host::ronnie.sv.splunk.com	109864
source = source::/home/my_user/tutorialdata/mailsv/secure.log	9829
source = source::/home/my_user/tutorialdata/vendor_sales/vendor_sales.log	30244
source = source::/home/my_user/tutorialdata/www1/access.log	13628
source = source::/home/my_user/tutorialdata/www1/secure.log	10593
source = source::/home/my_user/tutorialdata/www2/access.log	12912
source = source::/home/my_user/tutorialdata/www2/secure.log	9683
source = source::/home/my_user/tutorialdata/www3/access.log	12992
source = source::/home/my_user/tutorialdata/www3/secure.log	9983
sourcetype = sourcetype::access_combined_wcookie	39532
sourcetype = sourcetype::secure	40088
sourcetype = sourcetype::vendor_sales	30244
</pre></samp>

<h2 id="hive">Usage with Hive</h2>

<p>You can also use Bucket Reader to load Splunk data into Hive. The deserializer class <b><kbd>com.splunk.journal.hive.JournalSerDe</kbd></b> can be used to select columns, as well as handling the output format of the rows. For details, see <a href="?https://github.com/rzzldzzl/bucket_reader/blob/master/javadoc/com/splunk/journal/hive/JournalSerDe.html" target="_blank">the javadoc for this class</a>.</p>

<p>To use Bucket Reader with Hive, you must make tell Hive where to find the Bucket Reader jar. One way to do this is by invoking Hive with the "<kbd>--auxpath</kbd>" argument, like this:</p>

<pre><code>hive --auxpath /home/hive/splunk-bucket-reader-1.2.h1.jar</code></pre>

<p>In the following example, we create a table and load a Splunk journal file into it. Note that the columns specified as SerDe properties must match the columns used to define the table.</p>

<pre><code>CREATE TABLE splunk_event_table (
    Time             DATE,
    Host             STRING,
    Source           STRING,
    date_wday        STRING,
    date_mday        INT
)
ROW FORMAT SERDE 'com.splunk.journal.hive.JournalSerDe'
WITH SERDEPROPERTIES (
    "com.splunk.journal.hadoop.value_format" = "_time,host,source,date_wday,date_mday",
    "com.splunk.journal.hadoop.lenient"="true"
)
STORED AS INPUTFORMAT 'com.splunk.journal.hadoop.mapred.JournalInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

LOAD DATA LOCAL INPATH './journal.gz' OVERWRITE INTO TABLE splunk_event_table;
</code></pre>

            <p>After successfully running the commands shown above, you should check that your table contains the data
                you expected. Loading a journal file that contained data from the Splunk tutorial, leads to the following:</p>

<samp><pre>
hive> select * from splunk_event_table limit 10;
OK
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
2014-10-14	host::ronnie.sv.splunk.com	source::tutorialdata.zip:./tutorialdata/mailsv/secure.log	tuesday	14
Time taken: 0.119 seconds, Fetched: 10 row(s)
hive>

</pre></samp>


            <h2 id="pig">Usage with Pig</h2>

            <p>You can use Bucket Reader to read Splunk journal data using Apache Pig. The included class
                <b><kbd>com.splunk.journal.hive.JournalLoadFunc</kbd></b> wraps Bucket Reader and makes it available to
                Pig.</p>

            <p>To use Bucket Reader with Pig, you first register the Bucket Reader jar with your Pig command line.
                Then use the Pig Latin statement for loading data from a journal file, using JournalLoadFunc as a
                loader. The constructor for JournalLoadFunc takes as arguments the names of the fields you wish to load.
                These should be index-time fields that are present in the journal file or files you are working with.</p>

            <p>As an example, the following Pig Latin statements will count the number of events corresponding to each host:</p>

<samp><pre>
REGISTER splunk-bucket-reader-1.2.h2.jar;
A = LOAD 'journal.gz' USING com.splunk.journal.pig.JournalLoadFunc('host', 'source', '_time') AS (host:chararray, source:chararray, time:long);
B = GROUP A BY host;
C = FOREACH B GENERATE group, COUNT(A);
dump C;
</pre></samp>
</body></html>
